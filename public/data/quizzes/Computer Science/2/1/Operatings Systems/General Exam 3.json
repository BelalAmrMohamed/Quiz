{
  "meta": {
    "title": "General Exam 3",
    "id": "7BHAWERX",
    "createdAt": "2026-03-01 - 20:37",
    "path": "quizzes/Computer Science/2/1/Operatings Systems/General Exam 3.json"
  },
  "stats": {
    "questionCount": 30,
    "questionTypes": [
      "Essay",
      "MCQ"
    ]
  },
  "questions": [
    {
      "q": "Which condition necessarily requires preemptive CPU scheduling?",
      "explanation": "A transition from waiting to ready may cause the currently running process to be preempted in preemptive scheduling.",
      "options": [
        "A process terminates",
        "A process switches from running to waiting",
        "A process switches from waiting to ready",
        "A process performs I/O"
      ],
      "correct": 2
    },
    {
      "q": "The dispatcher is directly responsible for:",
      "explanation": "The dispatcher performs the context switch, switches to user mode, and jumps to the correct instruction.",
      "options": [
        "Selecting the next process to execute",
        "Saving and restoring memory mappings",
        "Context switching and transferring control to user mode",
        "Managing the ready queue"
      ],
      "correct": 2
    },
    {
      "q": "Which scheduling algorithm is provably optimal in minimizing average waiting time, assuming exact knowledge of CPU burst lengths?",
      "explanation": "SJF minimizes average waiting time if CPU burst lengths are known in advance.",
      "options": [
        "First-Come, First-Served (FCFS)",
        "Round Robin",
        "Priority Scheduling",
        "Shortest Job First (SJF)"
      ],
      "correct": 3
    },
    {
      "q": "The convoy effect is most likely to occur under:",
      "explanation": "FCFS can cause short processes to wait behind long CPU-bound processes.",
      "options": [
        "Round Robin scheduling",
        "Preemptive Shortest Job First",
        "First-Come, First-Served scheduling",
        "Priority scheduling with aging"
      ],
      "correct": 2
    },
    {
      "q": "In exponential averaging for CPU burst prediction, a value of α = 1 implies:",
      "explanation": "When α = 1, the next prediction depends only on the most recent CPU burst.",
      "options": [
        "All past bursts have equal weight",
        "Only the most recent burst is considered",
        "Burst history is ignored",
        "The prediction is constant"
      ],
      "correct": 1
    },
    {
      "q": "Which scheduling metric is most critical for interactive systems?",
      "explanation": "Interactive systems prioritize fast initial response to user requests.",
      "options": [
        "Turnaround time",
        "Waiting time",
        "Throughput",
        "Response time"
      ],
      "correct": 3
    },
    {
      "q": "Starvation in priority scheduling can be prevented using:",
      "explanation": "Aging gradually increases the priority of waiting processes.",
      "options": [
        "Preemption",
        "Time quantum",
        "Aging",
        "Context switching"
      ],
      "correct": 2
    },
    {
      "q": "Which statement best describes Round Robin scheduling?",
      "explanation": "Round Robin allocates CPU time fairly using fixed time slices.",
      "options": [
        "It is non-preemptive and priority-based",
        "It assigns CPU based on shortest burst",
        "It provides fairness using time quanta",
        "It minimizes turnaround time"
      ],
      "correct": 2
    },
    {
      "q": "A process differs from a program because a process:",
      "explanation": "A process is an active entity with state, registers, and a program counter.",
      "options": [
        "Is stored on disk",
        "Is passive",
        "Has an execution context",
        "Cannot be scheduled"
      ],
      "correct": 2
    },
    {
      "q": "Which component stores CPU registers, program counter, and scheduling information?",
      "explanation": "The PCB contains all information needed to manage and resume a process.",
      "options": [
        "Ready Queue",
        "Process Stack",
        "Process Control Block",
        "Kernel Stack"
      ],
      "correct": 2
    },
    {
      "q": "Which process state represents a process waiting for CPU allocation?",
      "explanation": "A ready process is waiting to be assigned the CPU.",
      "options": [
        "New",
        "Waiting",
        "Ready",
        "Running"
      ],
      "correct": 2
    },
    {
      "q": "A context switch is considered overhead because:",
      "explanation": "During a context switch, the CPU performs administrative work only.",
      "options": [
        "It executes user code",
        "It performs useful computation",
        "No user process makes progress",
        "It improves CPU utilization"
      ],
      "correct": 2
    },
    {
      "q": "Which system call replaces a process’s memory image with a new program?",
      "explanation": "exec() loads a new program into the existing process address space.",
      "options": [
        "fork()",
        "wait()",
        "exec()",
        "exit()"
      ],
      "correct": 2
    },
    {
      "q": "A process that has terminated but whose parent has not yet called wait() is called:",
      "explanation": "Zombie processes remain until the parent collects their exit status.",
      "options": [
        "Orphan",
        "Zombie",
        "Daemon",
        "Kernel process"
      ],
      "correct": 1
    },
    {
      "q": "Which IPC model requires explicit synchronization by user processes?",
      "explanation": "Shared memory requires synchronization mechanisms such as semaphores.",
      "options": [
        "Message passing",
        "Pipes",
        "Shared memory",
        "Signals"
      ],
      "correct": 2
    },
    {
      "q": "Which threading model allows true parallelism on multicore systems?",
      "explanation": "Each user thread maps to a kernel thread, enabling parallel execution.",
      "options": [
        "Many-to-One",
        "One-to-One",
        "User-level only",
        "Green threads"
      ],
      "correct": 1
    },
    {
      "q": "Which benefit of multithreading refers to lower overhead compared to processes?",
      "explanation": "Threads are cheaper to create and switch than processes.",
      "options": [
        "Scalability",
        "Responsiveness",
        "Economy",
        "Parallelism"
      ],
      "correct": 2
    },
    {
      "q": "Amdahl’s Law highlights the performance limitation caused by:",
      "explanation": "The serial fraction of a program limits overall speedup.",
      "options": [
        "Context switching",
        "Parallel overhead",
        "Serial portions of a program",
        "I/O wait time"
      ],
      "correct": 2
    },
    {
      "q": "Which OpenMP directive parallelizes a loop?",
      "explanation": "The parallel for directive distributes loop iterations across threads.",
      "options": [
        "#pragma omp task",
        "#pragma omp section",
        "#pragma omp parallel for",
        "#pragma omp critical"
      ],
      "correct": 2
    },
    {
      "q": "Which Grand Central Dispatch queue guarantees FIFO execution with no concurrency?",
      "explanation": "The main queue is a serial queue that executes tasks in FIFO order.",
      "options": [
        "Global concurrent queue",
        "High-priority queue",
        "Main queue",
        "Thread pool queue"
      ],
      "correct": 2
    },
    {
      "q": "Define CPU Scheduling and explain why it is essential in multiprogrammed operating systems.",
      "explanation": "CPU scheduling ensures efficient CPU utilization by allowing multiple processes to share the processor. It improves system throughput, fairness, and responsiveness by preventing the CPU from remaining idle when runnable processes exist.",
      "answer": "CPU scheduling is the mechanism by which the operating system selects one process from the ready queue to allocate the CPU."
    },
    {
      "q": "Differentiate between preemptive and non-preemptive scheduling. Give one implication of each.",
      "explanation": "Preemptive scheduling improves responsiveness and fairness but increases overhead and synchronization complexity. Non-preemptive scheduling reduces overhead but may lead to long waiting times and poor responsiveness.",
      "answer": "Preemptive scheduling allows the OS to interrupt a running process, while non-preemptive scheduling does not."
    },
    {
      "q": "Explain the role of the Process Control Block (PCB) in context switching.",
      "explanation": "During a context switch, the operating system saves the current process state into its PCB and restores another process’s state from its PCB. This enables correct suspension and resumption of processes.",
      "answer": "The PCB stores all necessary information to resume a process after interruption."
    },
    {
      "q": "Compare FCFS and Shortest Job First scheduling in terms of performance and limitations.",
      "explanation": "FCFS can suffer from the convoy effect, leading to poor performance. SJF provides optimal average waiting time but requires accurate prediction of CPU burst lengths, which is difficult in practice.",
      "answer": "FCFS is simple, while SJF minimizes average waiting time."
    },
    {
      "q": "Explain the four conditions under which CPU scheduling decisions occur.",
      "explanation": "Scheduling decisions occur when a process switches from running to waiting, running to ready, waiting to ready, or terminates. Only some of these transitions require preemption.",
      "answer": "Scheduling decisions occur during process state transitions."
    },
    {
      "q": "Define a thread and explain how threads improve performance compared to processes.",
      "explanation": "Threads improve performance by enabling parallelism, reducing context-switch overhead, and allowing shared memory access within a process. They are more lightweight than processes.",
      "answer": "A thread is the smallest unit of CPU execution within a process."
    },
    {
      "q": "Compare Many-to-One and One-to-One threading models.",
      "explanation": "Many-to-One does not allow true parallelism and suffers from blocking issues. One-to-One enables parallelism on multicore systems but incurs higher overhead.",
      "answer": "Many-to-One maps many user threads to one kernel thread, while One-to-One maps each user thread to a kernel thread."
    },
    {
      "q": "Explain Amdahl’s Law and its implication for multicore systems.",
      "explanation": "Amdahl’s Law states that the serial portion of a program limits overall speedup. This emphasizes optimizing or reducing serial code to benefit from multicore architectures.",
      "answer": "Amdahl’s Law limits the speedup achievable through parallelism."
    },
    {
      "q": "Describe the Shared Memory IPC model, including one advantage and one challenge.",
      "explanation": "Shared memory provides fast communication since no kernel intervention is required after setup. However, it requires explicit synchronization to avoid race conditions.",
      "answer": "Shared memory allows processes to communicate via a common memory region."
    },
    {
      "q": "Explain why dual-mode operation is critical for operating system protection.",
      "explanation": "Dual-mode operation prevents user programs from executing privileged instructions, protecting the OS from misuse and ensuring system stability and security.",
      "answer": "Dual-mode operation separates user and kernel execution."
    }
  ]
}